# Analyzing Objective Functions and Optimization Methods in Classification Task

In this project I implemented various optimization methods and loss functions for classification tasks. By applying the different loss functions and optimization techniques across different datasets, I gained an understanding of the efficiency vs. accuracy tradeoff of each model, and under which conditions they thrive.

### Loss Functions:
1) **Hinge Loss**: Non-smooth: Use sub-gradient optimization 

2) **Hinge Loss** (Smooth Approximation): Use only optimization methods that only apply to smooth functions

3) **L2 Penalized Logistic Regression**: Use only optimization methods that only apply to smooth functions

### Optimization Methods:
i) Stochastic sub-gradient

ii) Stochastic gradient

iii) Mini-batch (sub-)gradient

iv) Stochastic average sub-gradient (SAG)

v) Stochastic average gradient (SAG)

vi) Gradient descent with Armijo line-search

vii) Acceleratd gradient with Armijo line-search

